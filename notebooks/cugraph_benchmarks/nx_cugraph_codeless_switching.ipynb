{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Performance of NetworkX with Rapids GPU-based nx_cugraph backend vs on cpu\n",
    "# Skip notebook test\n",
    "This notebook demonstrates compares the performance of nx_cugraph as a dispatcher for NetworkX algorithms. \n",
    "\n",
    "We do this by executing Betweenness Centrality, Breadth First Search and Louvain Community Detection, collecting run times with and without nx_cugraph backend and graph caching enabled. nx_cugraph is a registered NetworkX backend. Using it is a zero code change solution.\n",
    "\n",
    "In the notebook switching to the nx-cugraph backend is done via variables set using the [NetworkX config package](https://networkx.org/documentation/stable/reference/backends.html#networkx.utils.configs.NetworkXConfig) **which requires networkX 3.3 or later !!**\n",
    "\n",
    "\n",
    "They can be set at the command line as well.\n",
    "\n",
    "### See this example from GTC Spring 2024\n",
    "\n",
    "\n",
    "\n",
    "Here is a sample minimal script to demonstrate No-code-change GPU acceleration using nx-cugraph.\n",
    "\n",
    "----\n",
    "bc_demo.ipy:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "url = \"https://data.rapids.ai/cugraph/datasets/cit-Patents.csv\"\n",
    "df = pd.read_csv(url, sep=\" \", names=[\"src\", \"dst\"], dtype=\"int32\")\n",
    "G = nx.from_pandas_edgelist(df, source=\"src\", target=\"dst\")\n",
    "\n",
    "%time result = nx.betweenness_centrality(G, k=10)\n",
    "```\n",
    "----\n",
    "Running it with the nx-cugraph backend looks like this:\n",
    "```\n",
    "user@machine:/# ipython bc_demo.ipy\n",
    "CPU times: user 7min 38s, sys: 5.6 s, total: 7min 44s\n",
    "Wall time: 7min 44s\n",
    "\n",
    "user@machine:/# NETWORKX_BACKEND_PRIORITY=cugraph ipython bc_demo.ipy\n",
    "CPU times: user 18.4 s, sys: 1.44 s, total: 19.9 s\n",
    "Wall time: 20 s\n",
    "```\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This installs the NetworkX cuGraph dispatcher if not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import nx_cugraph\n",
    "except ModuleNotFoundError:\n",
    "    os.system('conda install -c rapidsai -c conda-forge -c nvidia nx-cugraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is boiler plate NetworkX code to run:\n",
    "* betweenness Centrality\n",
    "* Bredth first Search\n",
    "* Louvain community detection\n",
    "\n",
    "and collect times. it is completely unaware of cugraph or GPU-based tools.\n",
    "[NetworkX configurations](https://networkx.org/documentation/stable/reference/utils.html#backends) can determine how they are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algos(G):\n",
    "    print()\n",
    "    print (\"Betweenness Centrality time\")\n",
    "    %time\n",
    "    result = nx.betweenness_centrality(G, k=10)\n",
    "    print()\n",
    "    print (\"Breadth First Search time\")\n",
    "    %time\n",
    "    result = nx.bfs_tree(G,source=1)\n",
    "    print()\n",
    "    print (\"Louvain time\")\n",
    "    %time\n",
    "    result = nx.community.louvain_communities(G,threshold=1e-04)\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads a patent citation dataset containing 3774768 nodes and 16518948 edges and loads it into a NetworkX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./data/cit-Patents.csv\"\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    print(\"File found\")\n",
    "    url = filepath\n",
    "else:\n",
    "    url = \"https://data.rapids.ai/cugraph/datasets/cit-Patents.csv\"\n",
    "df = pd.read_csv(url, sep=\" \", names=[\"src\", \"dst\"], dtype=\"int32\")\n",
    "G = nx.from_pandas_edgelist(df, source=\"src\", target=\"dst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the NetworkX dispatcher with an environment variable or in code using NetworkX config package which is new to [NetworkX 3.3 config](https://networkx.org/documentation/stable/reference/backends.html#networkx.utils.configs.NetworkXConfig).\n",
    "\n",
    "These convenience settinge allow turning off caching and cugraph dispatching if you want to see how long cpu-only would take.\n",
    "In this example using an AMD Ryzen Threadripper PRO 3975WX 32-Cores cpu raised the run time to over 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cugraph = True\n",
    "cache_graph = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cugraph:\n",
    "    nx.config[\"backend_priority\"]=['cugraph']\n",
    "else:\n",
    "    # Use this setting to turn off the cugraph dispatcher running in legacy cpu mode.\n",
    "    nx.config[\"backend_priority\"]=[]\n",
    "if cache_graph:\n",
    "    nx.config[\"cache_converted_graphs\"]= True\n",
    "else:\n",
    "    # Use this setting to turn off graph caching which will convertthe NetworkX to a gpu-resident graph each time an algorithm is run.\n",
    "    nx.config[\"cache_converted_graphs\"]= False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithms on GPU. \n",
    "\n",
    "**Note the messages NetworkX generates to remind us cached graph shouldn't be modified.**\n",
    "\n",
    "```\n",
    "For the cache to be consistent (i.e., correct), the input graph must not have been manually mutated since the cached graph was created.\n",
    "\n",
    "Using cached graph for 'cugraph' backend in call to bfs_edges.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Betweenness Centrality time\n",
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 9.3 µs\n",
      "\n",
      "Breadth First Search time\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacosta/anaconda3/envs/cugraph_24_06_nx_run/lib/python3.11/site-packages/networkx/utils/backends.py:1101: UserWarning: Using cached graph for 'cugraph' backend in call to bfs_edges.\n",
      "\n",
      "For the cache to be consistent (i.e., correct), the input graph must not have been manually mutated since the cached graph was created. Examples of manually mutating the graph data structures resulting in an inconsistent cache include:\n",
      "\n",
      "    >>> G[u][v][key] = val\n",
      "\n",
      "and\n",
      "\n",
      "    >>> for u, v, d in G.edges(data=True):\n",
      "    ...     d[key] = val\n",
      "\n",
      "Using methods such as `G.add_edge(u, v, weight=val)` will correctly clear the cache to keep it consistent. You may also use `G.__networkx_cache__.clear()` to manually clear the cache, or set `G.__networkx_cache__` to None to disable caching for G. Enable or disable caching via `nx.config.cache_converted_graphs` config.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Louvain time\n",
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n",
      "\n",
      "Total Algorithm run time\n",
      "CPU times: user 1min 26s, sys: 6.63 s, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_algos(G)\n",
    "print (\"Total Algorithm run time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Copyright (c) 2024, NVIDIA CORPORATION.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
